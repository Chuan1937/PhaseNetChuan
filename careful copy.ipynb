{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.models as models\n",
    "import tensorflow.keras.activations as activations\n",
    "import tensorflow.keras.metrics as metrics\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow_addons.optimizers as optimizers\n",
    "import tensorflow_addons.losses as losses\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "# Loads dataset.\n",
    "# Targets are the segmantation images.\n",
    "# Inputs are the original images.\n",
    "def load_dataset(dataset_path):\n",
    "    data = np.load(dataset_path, allow_pickle=True)\n",
    "    return data['inputs'], data['targets']\n",
    "\n",
    "\n",
    "# Defining the encoder's down-sampling blocks.\n",
    "def encoder_block(inputs, n_filters, kernel_size, strides):\n",
    "    encoder = layers.Conv2D(filters=n_filters, kernel_size=kernel_size, strides=strides, padding='same', use_bias=False)(inputs)\n",
    "    encoder = layers.BatchNormalization()(encoder)\n",
    "    encoder = layers.Activation(activations.gelu)(encoder)\n",
    "    encoder = layers.Conv2D(filters=n_filters, kernel_size=kernel_size, padding='same', use_bias=False)(encoder)\n",
    "    encoder = layers.BatchNormalization()(encoder)\n",
    "    encoder = layers.Activation(activations.gelu)(encoder)\n",
    "    return encoder\n",
    "\n",
    "\n",
    "# 定义解码器的上采样块。\n",
    "def upscale_blocks(inputs):\n",
    "    n_upscales = len(inputs)\n",
    "    upscale_layers = []\n",
    "\n",
    "    for i, inp in enumerate(inputs):\n",
    "        p = n_upscales - i\n",
    "        u = layers.Conv2DTranspose(filters=64, kernel_size=3, strides=2**p, padding='same')(inp)\n",
    "\n",
    "        for i in range(2):\n",
    "            u = layers.Conv2D(filters=64, kernel_size=3, padding='same', use_bias=False)(u)\n",
    "            u = layers.BatchNormalization()(u)\n",
    "            u = layers.Activation(activations.gelu)(u)\n",
    "            u = layers.Dropout(rate=0.4)(u)\n",
    "\n",
    "        upscale_layers.append(u)\n",
    "    return upscale_layers\n",
    "\n",
    "\n",
    "# 定义解码器的整个模块。\n",
    "\n",
    "def decoder_block(layers_to_upscale, inputs):\n",
    "    upscaled_layers = upscale_blocks(layers_to_upscale)\n",
    "\n",
    "    decoder_blocks = []\n",
    "\n",
    "    for i, inp in enumerate(inputs):\n",
    "        d = layers.Conv2D(filters=64, kernel_size=3, strides=2**i, padding='same', use_bias=False)(inp)\n",
    "        d = layers.BatchNormalization()(d)\n",
    "        d = layers.Activation(activations.gelu)(d)\n",
    "        d = layers.Conv2D(filters=64, kernel_size=3, padding='same', use_bias=False)(d)\n",
    "        d = layers.BatchNormalization()(d)\n",
    "        d = layers.Activation(activations.gelu)(d)\n",
    "\n",
    "        decoder_blocks.append(d)\n",
    "\n",
    "    decoder = layers.concatenate(upscaled_layers + decoder_blocks)\n",
    "    decoder = layers.Conv2D(filters=256, kernel_size=3, strides=1, padding='same', use_bias=False)(decoder)\n",
    "    decoder = layers.BatchNormalization()(decoder)\n",
    "    decoder = layers.Activation(activations.gelu)(decoder)\n",
    "    decoder = layers.Dropout(rate=0.4)(decoder)\n",
    "\n",
    "    return decoder\n",
    "\n",
    "\n",
    "def get_model(input_dim):\n",
    "    inputs = layers.Input(input_dim)\n",
    "\n",
    "    noisy_inputs = layers.GaussianNoise(stddev=0.2)(inputs)\n",
    "\n",
    "    e1 = encoder_block(noisy_inputs, n_filters=32, kernel_size=3, strides=1)\n",
    "    e2 = encoder_block(e1, n_filters=64, kernel_size=3, strides=2)\n",
    "    e3 = encoder_block(e2, n_filters=128, kernel_size=3, strides=2)\n",
    "    e4 = encoder_block(e3, n_filters=256, kernel_size=3, strides=2)\n",
    "    e5 = encoder_block(e4, n_filters=512, kernel_size=3, strides=2)\n",
    "\n",
    "    d4 = decoder_block(layers_to_upscale=[e5], inputs=[e4, e3, e2, e1])\n",
    "    d3 = decoder_block(layers_to_upscale=[e5, d4], inputs=[e3, e2, e1])\n",
    "    d2 = decoder_block(layers_to_upscale=[e5, d4, d3], inputs=[e2, e1])\n",
    "    d1 = decoder_block(layers_to_upscale=[e5, d4, d3, d2], inputs=[e1])\n",
    "\n",
    "    output = layers.Conv2D(filters=3, kernel_size=1, padding='same', activation='tanh')(d1)\n",
    "\n",
    "    model = models.Model(inputs, output)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model((256, 256, 3))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Loading the dataset.\n",
    "# x_train: Segmentation images.\n",
    "# y_train: Original images.\n",
    "DATASET_PATH = 'D:\\\\Datasets\\\\City-Segmentation\\\\Cityscapes\\\\dataset.npz'\n",
    "x_train, y_train = load_dataset(DATASET_PATH)\n",
    "\n",
    "# Normalizing data.\n",
    "x_train = (x_train - 127.5) / 127.5\n",
    "y_train = (y_train - 127.5) / 127.5\n",
    "\n",
    "# Building & Compiling the model.\n",
    "image_size = x_train[0].shape\n",
    "unet3_plus = get_model(image_size)\n",
    "\n",
    "unet3_plus.compile(\n",
    "    optimizer=optimizers.Yogi(learning_rate=0.00025),\n",
    "    loss=losses.sigmoid_focal_crossentropy,\n",
    "    metrics=[metrics.MeanIoU(num_classes=30)]\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "epochs = 200\n",
    "batch_size = 4\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(buffer_size=x_train.shape[0])\n",
    "inputs = dataset.batch(batch_size=batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "batches_per_epoch = x_train.shape[0] // batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('\\nTraining on epoch', epoch + 1)\n",
    "\n",
    "    loss = 0\n",
    "    for i, (x_batch, y_batch) in enumerate(inputs):\n",
    "        loss = unet3_plus.train_on_batch(x_batch, y_batch)\n",
    "\n",
    "        print('\\rCurrent batch: {}/{} , loss = {}'.format(\n",
    "            i+1,\n",
    "            batches_per_epoch,\n",
    "            loss, end='')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
